<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>NLP Course Portfolio | Kanishk</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>

  <header>
    <h1>Natural Language Processing (21CSC356T)</h1>
    <p>Course Portfolio | SRM Institute of Science and Technology</p>
    <p>By: Kanishk Reddy - RA2211003010870</p>
  </header>

  <nav>
    <a href="#about">About</a>
    <a href="#abstract">Abstract</a>
    <a href="#unit1">Unit 1</a>
    <a href="#unit2">Unit 2</a>
    <a href="#unit3">Unit 3</a>
    <a href="#unit4">Unit 4</a>
    <a href="#unit5">Unit 5</a>
  </nav>

  <main>
    <section id="about" class="fade-section">
      <h2>About the Course</h2>
      <p>
        <b>Course Code:</b> 21CSC356T <br>
        <b>Course Title:</b> Natural Language Processing (Elective) <br>
        <b>Instructor:</b> Dr. Viveka S <br>
        <b>Department:</b> Computing Technologies, SRMIST, Kattankulathur <br>
      </p>
      <p>
        This elective course introduces the foundations of Natural Language Processing (NLP) — the intersection
        of computer science, linguistics, and artificial intelligence. The course covers both the traditional
        statistical methods and modern deep learning approaches for processing and understanding human language.
      </p>
      <p>
        The curriculum is divided into five comprehensive units that span from language fundamentals
        to advanced transformer-based architectures and practical applications such as chatbots,
        information retrieval, and summarization.
      </p>
    </section>

    <section id="abstract" class="fade-section">
      <h2>Abstract</h2>
      <p>
        This portfolio presents a structured summary of the five major units covered in the course.
        <b>Unit 1</b> introduces the basic concepts of NLP and text preprocessing methods.
        <b>Unit 2</b> explores syntactic parsing using grammar-based models.
        <b>Unit 3</b> delves into semantics, lexical relations, and word embeddings.
        <b>Unit 4</b> transitions into deep learning-based models including RNNs, LSTMs, and Transformers.
        Finally, <b>Unit 5</b> focuses on real-world NLP applications such as chatbots,
        question answering, summarization, and machine translation.
      </p>
      <p>
        Through this course, I learned how to convert raw unstructured text into meaningful
        representations that enable computational understanding. I also explored modern tools and
        architectures that power today’s AI-driven language technologies.
      </p>
    </section>

    <!-- UNIT 1 -->
    <section id="unit1" class="fade-section">
      <h2>Unit 1 — Introduction to Natural Language Processing</h2>
      <h3>Topics Covered</h3>
      <p>
        This unit introduced the fundamental concepts of NLP, its goals, and real-world applications
        in areas like sentiment analysis, chatbots, and information retrieval. I studied the various
        levels of language processing — phonology, morphology, syntax, semantics, and pragmatics —
        which form the basis of language understanding.
      </p>
      <p>
        I also learned about text preprocessing techniques: tokenization, stemming, lemmatization,
        and feature extraction methods such as Term Frequency (TF), Inverse Document Frequency (IDF),
        and TF-IDF modeling. These techniques help transform raw text into numerical representations
        suitable for computational models.
      </p>
      <h3>What I Learned</h3>
      <p>
        This unit built my foundation in NLP and helped me understand how textual data is prepared
        before being used for analysis or modeling. I gained practical insights into cleaning,
        normalizing, and representing text — essential for every downstream NLP task.
      </p>
    </section>

    <!-- UNIT 2 -->
    <section id="unit2" class="fade-section">
      <h2>Unit 2 — Syntax and Parsing</h2>
      <h3>Topics Covered</h3>
      <p>
        This unit focused on understanding sentence structure using grammatical formalisms.
        I studied Context-Free Grammars (CFGs), top-down and bottom-up parsing techniques,
        CKY and Earley parsers, and how ambiguity in language can be resolved.
      </p>
      <p>
        I also explored dependency parsing and probabilistic grammars that extend CFGs to handle
        real-world linguistic variations. These techniques are used in applications like grammar
        correction and syntactic analysis.
      </p>
      <h3>What I Learned</h3>
      <p>
        I learned how syntactic parsing is crucial for understanding sentence meaning and structure.
        Implementing parsing algorithms gave me a deeper appreciation for how computers analyze
        sentence patterns and relationships between words.
      </p>
    </section>

    <!-- UNIT 3 -->
    <section id="unit3" class="fade-section">
      <h2>Unit 3 — Semantics and Lexical Meaning</h2>
      <h3>Topics Covered</h3>
      <p>
        The third unit explored how meaning is represented and computed. Topics included
        lexical semantics, word senses, semantic relations, and Word Sense Disambiguation (WSD).
        The unit also covered word embeddings like Word2Vec (CBOW and Skip-gram models) and GloVe.
      </p>
      <p>
        I also learned about discourse-level concepts such as coherence, reference resolution,
        and coreference — key elements in understanding extended text beyond individual sentences.
      </p>
      <h3>What I Learned</h3>
      <p>
        This unit helped me understand how meaning is encoded in vector spaces and how embeddings
        capture contextual similarity. I gained clarity on how semantic relationships support
        tasks like sentiment analysis, translation, and topic modeling.
      </p>
    </section>

    <!-- UNIT 4 -->
    <section id="unit4" class="fade-section">
      <h2>Unit 4 — Deep Learning for NLP</h2>
      <h3>Topics Covered</h3>
      <p>
        This unit transitioned into modern deep learning models used in NLP. I studied Recurrent Neural Networks (RNNs),
        Long Short-Term Memory (LSTM) networks, attention mechanisms, and Transformer architectures.
        We explored BERT and RoBERTa models and their fine-tuning for various downstream tasks such as text classification and generation.
      </p>
      <h3>What I Learned</h3>
      <p>
        I gained an understanding of how neural architectures learn contextual dependencies in language.
        Learning about Transformers and self-attention mechanisms made me realize how these models
        overcome the limitations of RNNs and have become the backbone of today’s NLP systems.
      </p>
    </section>

    <!-- UNIT 5 -->
    <section id="unit5" class="fade-section">
      <h2>Unit 5 — NLP Applications</h2>
      <h3>Topics Covered</h3>
      <p>
        The final unit demonstrated how NLP techniques are applied in real-world systems.
        I studied chatbot architectures (retrieval-based and generative), information extraction,
        semantic search, question answering, and text summarization (extractive vs. abstractive).
        The unit concluded with an overview of machine translation systems.
      </p>
      <h3>What I Learned</h3>
      <p>
        This unit connected all the previous concepts and helped me understand the practical
        implementation of NLP models. I now appreciate how theory translates into interactive
        systems that interpret, generate, and respond to human language.
      </p>
    </section>
  </main>

  <footer>
    <p>© 2025 Kanishk | NLP Course Portfolio | SRMIST</p>
  </footer>

  <script src="script.js"></script>
</body>
</html>
